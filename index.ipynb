{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_grade(x):\n",
    "    '''\n",
    "    This is a helper function for load_and_format_x_y()\n",
    "    This function takes in a dataframe of x predictors. \n",
    "    Initiates a OneHotEncoder with handle_unknown=\"ignore\" to \n",
    "    compensate for unseen artists in new data, \n",
    "    And returns the dataframe with the artist column encoded.\n",
    "    '''\n",
    "    # initiate OHE\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\")\n",
    "    # dataframe of just encoded columns, using x_train index as it's index to ensure rows line up\n",
    "    ohe.fit(x[['grade']])\n",
    "    grade_dummies = pd.DataFrame(ohe.transform(x[['grade']]).todense(), columns=ohe.get_feature_names(), index=x.index)\n",
    "    # concat encoded columns to x\n",
    "    concatted_x = pd.concat([x, grade_dummies], axis=1)\n",
    "    #add column names\n",
    "    grade_nums = [1,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    grades = []\n",
    "    for num in grade_nums:\n",
    "        grades.append(f'grade_{num}')\n",
    "    concatted_x.columns = list(x.columns) + list(grades)\n",
    "    concatted_x = concatted_x.drop('grade', axis=1)\n",
    "    return concatted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_grade_all(x):\n",
    "    '''\n",
    "    This is a helper function for load_and_format_x_y()\n",
    "    This function takes in a dataframe of x predictors. \n",
    "    Initiates a OneHotEncoder with handle_unknown=\"ignore\" to \n",
    "    compensate for unseen artists in new data, \n",
    "    And returns the dataframe with the artist column encoded.\n",
    "    '''\n",
    "    \n",
    "    # initiate OHE\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\")\n",
    "    # dataframe of just encoded columns, using x_train index as it's index to ensure rows line up\n",
    "    ohe.fit(x[['grade']])\n",
    "    grade_dummies = pd.DataFrame(ohe.transform(x[['grade']]).todense(), columns=ohe.get_feature_names(), index=x.index)\n",
    "    # concat encoded columns to x\n",
    "    concatted_x = pd.concat([x, grade_dummies], axis=1)\n",
    "    #add column names\n",
    "    grade_nums = list(x['grade'].value_counts().index)\n",
    "    grades = []\n",
    "    for num in grade_nums:\n",
    "        grades.append(f'grade_{num}')\n",
    "    concatted_x.columns = list(x.columns) + list(grades)\n",
    "    concatted_x = concatted_x.drop('grade', axis=1)\n",
    "    return concatted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data = encode_grade_all(data)\n",
    "    data = data.drop('Unnamed: 0', axis=1)\n",
    "    data['ratio'] = data['sqft_living']/data['sqft_lot']\n",
    "    x = data.drop(['zipcode', 'lat', 'long'], axis=1)\n",
    "    x = x.set_index('id')\n",
    "    x = x.drop('date', axis=1)\n",
    "    x_cols = x.columns\n",
    "    ss = StandardScaler()\n",
    "    x = pd.DataFrame(ss.fit_transform(x))\n",
    "    x.columns = x_cols\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('kc_house_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['price']\n",
    "x = preprocess_data(data.drop('price', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 124322210290.3152           35.53s\n",
      "         2 111701008476.3752           39.46s\n",
      "         3 101254208302.4301           36.24s\n",
      "         4 92475683471.5541           34.18s\n",
      "         5 84937246168.4440           32.40s\n",
      "         6 78693018236.1423           30.72s\n",
      "         7 73249783387.2247           29.80s\n",
      "         8 68531836209.5698           29.46s\n",
      "         9 64579131671.8702           28.75s\n",
      "        10 61117977502.4731           28.31s\n",
      "        20 42346326683.0487           24.34s\n",
      "        30 35765083784.5238           23.67s\n",
      "        40 32643219173.5307           23.59s\n",
      "        50 30932325714.3272           24.50s\n",
      "        60 29658480914.4235           23.85s\n",
      "        70 28870600578.5589           22.92s\n",
      "        80 28101710538.6183           22.42s\n",
      "        90 27551203148.4621           21.78s\n",
      "       100 27060794540.0503           21.40s\n",
      "       200 23772534729.7621           17.89s\n",
      "       300 21733006178.2155           11.64s\n",
      "       400 20283757737.9194            5.63s\n",
      "       500 19138320080.9455            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=3,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(n_estimators=500,  min_samples_split=3, verbose=1)\n",
    "gbr.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('kc_house_data_test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = preprocess_holdout(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>...</th>\n",
       "      <th>grade_9</th>\n",
       "      <th>grade_10</th>\n",
       "      <th>grade_6</th>\n",
       "      <th>grade_11</th>\n",
       "      <th>grade_5</th>\n",
       "      <th>grade_12</th>\n",
       "      <th>grade_4</th>\n",
       "      <th>grade_13</th>\n",
       "      <th>grade_1</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.676412</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.089644</td>\n",
       "      <td>-0.030766</td>\n",
       "      <td>-1.260349</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-0.442861</td>\n",
       "      <td>1.182544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>-0.748766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.676412</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.089644</td>\n",
       "      <td>-0.030766</td>\n",
       "      <td>-1.260349</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-0.442861</td>\n",
       "      <td>1.182544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>-0.748766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.427449</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>-0.737950</td>\n",
       "      <td>-0.269995</td>\n",
       "      <td>0.445769</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-0.867374</td>\n",
       "      <td>0.154414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>0.967037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.427449</td>\n",
       "      <td>-0.731556</td>\n",
       "      <td>-0.934504</td>\n",
       "      <td>0.084898</td>\n",
       "      <td>-1.260349</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-1.012602</td>\n",
       "      <td>0.007539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>-1.071727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.676412</td>\n",
       "      <td>0.547262</td>\n",
       "      <td>0.668960</td>\n",
       "      <td>-0.113799</td>\n",
       "      <td>0.445769</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>0.998248</td>\n",
       "      <td>-0.604444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>-0.337154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>-0.427449</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>-0.675881</td>\n",
       "      <td>-0.285942</td>\n",
       "      <td>2.151888</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-0.454033</td>\n",
       "      <td>-0.604444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>2.403372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>0.676412</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.131024</td>\n",
       "      <td>-0.170720</td>\n",
       "      <td>0.445769</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>0.417336</td>\n",
       "      <td>-0.604444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>-0.203144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4320</th>\n",
       "      <td>-1.531311</td>\n",
       "      <td>-2.010374</td>\n",
       "      <td>-1.203472</td>\n",
       "      <td>-0.280553</td>\n",
       "      <td>0.445769</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-1.023773</td>\n",
       "      <td>-0.604444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>1.415687</td>\n",
       "      <td>-0.715589</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>0.774014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321</th>\n",
       "      <td>-0.427449</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>-0.603466</td>\n",
       "      <td>-0.255008</td>\n",
       "      <td>0.445769</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-0.375833</td>\n",
       "      <td>-0.604444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>-0.706371</td>\n",
       "      <td>1.397451</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>0.540648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>-1.531311</td>\n",
       "      <td>-2.010374</td>\n",
       "      <td>-1.203472</td>\n",
       "      <td>-0.287296</td>\n",
       "      <td>0.445769</td>\n",
       "      <td>-0.080742</td>\n",
       "      <td>-0.282951</td>\n",
       "      <td>-0.447707</td>\n",
       "      <td>-1.023773</td>\n",
       "      <td>-0.604444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>-0.25089</td>\n",
       "      <td>1.415687</td>\n",
       "      <td>-0.715589</td>\n",
       "      <td>-0.445537</td>\n",
       "      <td>-0.270131</td>\n",
       "      <td>-0.153883</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.015211</td>\n",
       "      <td>1.298918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4323 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront  \\\n",
       "0     0.676412   0.227557     0.089644 -0.030766 -1.260349   -0.080742   \n",
       "1     0.676412   0.227557     0.089644 -0.030766 -1.260349   -0.080742   \n",
       "2    -0.427449   0.227557    -0.737950 -0.269995  0.445769   -0.080742   \n",
       "3    -0.427449  -0.731556    -0.934504  0.084898 -1.260349   -0.080742   \n",
       "4     0.676412   0.547262     0.668960 -0.113799  0.445769   -0.080742   \n",
       "...        ...        ...          ...       ...       ...         ...   \n",
       "4318 -0.427449   0.227557    -0.675881 -0.285942  2.151888   -0.080742   \n",
       "4319  0.676412   0.227557     0.131024 -0.170720  0.445769   -0.080742   \n",
       "4320 -1.531311  -2.010374    -1.203472 -0.280553  0.445769   -0.080742   \n",
       "4321 -0.427449   0.227557    -0.603466 -0.255008  0.445769   -0.080742   \n",
       "4322 -1.531311  -2.010374    -1.203472 -0.287296  0.445769   -0.080742   \n",
       "\n",
       "          view  condition  sqft_above  sqft_basement  ...   grade_9  grade_10  \\\n",
       "0    -0.282951  -0.447707   -0.442861       1.182544  ... -0.087706  -0.25089   \n",
       "1    -0.282951  -0.447707   -0.442861       1.182544  ... -0.087706  -0.25089   \n",
       "2    -0.282951  -0.447707   -0.867374       0.154414  ... -0.087706  -0.25089   \n",
       "3    -0.282951  -0.447707   -1.012602       0.007539  ... -0.087706  -0.25089   \n",
       "4    -0.282951  -0.447707    0.998248      -0.604444  ... -0.087706  -0.25089   \n",
       "...        ...        ...         ...            ...  ...       ...       ...   \n",
       "4318 -0.282951  -0.447707   -0.454033      -0.604444  ... -0.087706  -0.25089   \n",
       "4319 -0.282951  -0.447707    0.417336      -0.604444  ... -0.087706  -0.25089   \n",
       "4320 -0.282951  -0.447707   -1.023773      -0.604444  ... -0.087706  -0.25089   \n",
       "4321 -0.282951  -0.447707   -0.375833      -0.604444  ... -0.087706  -0.25089   \n",
       "4322 -0.282951  -0.447707   -1.023773      -0.604444  ... -0.087706  -0.25089   \n",
       "\n",
       "       grade_6  grade_11   grade_5  grade_12   grade_4  grade_13   grade_1  \\\n",
       "0    -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "1    -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "2    -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "3    -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "4    -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4318 -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "4319 -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "4320  1.415687 -0.715589 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "4321 -0.706371  1.397451 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "4322  1.415687 -0.715589 -0.445537 -0.270131 -0.153883    -0.057 -0.015211   \n",
       "\n",
       "         ratio  \n",
       "0    -0.748766  \n",
       "1    -0.748766  \n",
       "2     0.967037  \n",
       "3    -1.071727  \n",
       "4    -0.337154  \n",
       "...        ...  \n",
       "4318  2.403372  \n",
       "4319 -0.203144  \n",
       "4320  0.774014  \n",
       "4321  0.540648  \n",
       "4322  1.298918  \n",
       "\n",
       "[4323 rows x 26 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(gbr_pca.predict(x_test))\n",
    "predictions.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 16 and input n_features is 26 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-8202f9fafd54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   2567\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m         \u001b[0;31m# In regression we can directly return the raw value from the trees.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2569\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;34m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m         \u001b[0mraw_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_predict_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m         predict_stages(self.estimators_, X, self.learning_rate,\n\u001b[1;32m   1657\u001b[0m                        raw_predictions)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_raw_predict_init\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0;34m\"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 16 and input n_features is 26 "
     ]
    }
   ],
   "source": [
    "gbr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('kc_house_data_train.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ratio'] = data['sqft_living']/data['sqft_lot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encode_grade(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(['price', 'zipcode', 'lat', 'long'], axis=1)\n",
    "y = data['price']\n",
    "# x_test = pd.read_csv('kc_house_data_test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.set_index('id')\n",
    "x = x.drop('date', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x = pd.DataFrame(ss.fit_transform(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6984464197541488"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "lr.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6766681138549764"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796222642000603"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(x_train, y_train)\n",
    "gbr.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68693847, 0.61560014, 0.71650872, 0.70406202, 0.71597532])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gbr, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "x_train_pca2 = pca.fit_transform(x_train2)\n",
    "x_test_pca2 = pca.fit_transform(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7590532270775023"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_pca= GradientBoostingRegressor()\n",
    "gbr_pca.fit(x_train_pca, y_train)\n",
    "gbr_pca.score(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68750301, 0.70388706, 0.61036695, 0.71219501, 0.66635662])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gbr_pca, x_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding optimal n_components for pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_feats = [10,8,6]\n",
    "# scores = []\n",
    "# for num in max_feats:\n",
    "#     gbr_pca= GradientBoostingRegressor(max_features=num)\n",
    "#     gbr_pca.fit(x_train_pca, y_train)\n",
    "#     scores.append((num, gbr_pca.score(x_train_pca, y_train)))\n",
    "#     print(f'done with {num} number of max feats')\n",
    "# scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 118959622480.1141           37.40s\n",
      "         2 107953701231.5737           32.29s\n",
      "         3 98448831066.4784           33.54s\n",
      "         4 90844066065.6932           33.21s\n",
      "         5 84217559224.3214           31.86s\n",
      "         6 78662534075.9526           30.71s\n",
      "         7 73557838899.4765           29.69s\n",
      "         8 69200955883.0480           28.81s\n",
      "         9 65605095437.9484           28.09s\n",
      "        10 62535719723.9858           27.23s\n",
      "        20 45070111006.2471           23.26s\n",
      "        30 37923762892.0710           22.71s\n",
      "        40 34295378844.6169           22.42s\n",
      "        50 31989931097.7658           21.46s\n",
      "        60 30594507519.0900           20.59s\n",
      "        70 29386421008.2003           19.99s\n",
      "        80 28459028416.0673           19.77s\n",
      "        90 27680470965.1230           19.19s\n",
      "       100 27063484060.3038           18.64s\n",
      "       200 23295273339.1070           14.18s\n",
      "       300 21093107176.1212           11.19s\n",
      "       400 19526878711.0607            5.51s\n",
      "       500 18198844780.1783            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.767641101986647"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_pca = GradientBoostingRegressor(n_estimators=500,  min_samples_split=3, verbose=1)\n",
    "gbr_pca.fit(x_train2, y_train2)\n",
    "gbr_pca.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 122875370273.7433            2.06s\n",
      "         2 110394404488.5574            1.46s\n",
      "         3 99936012577.2876            1.10s\n",
      "         4 91352467698.3215            0.85s\n",
      "         5 84058091973.6071            0.67s\n",
      "         6 77982836137.5191            0.52s\n",
      "         7 72842630477.9844            0.37s\n",
      "         8 68536118385.0893            0.24s\n",
      "         9 64812915566.8446            0.12s\n",
      "        10 61779931409.6912            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5509992720401198"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_pca_opt= GradientBoostingRegressor(criterion='mse', n_estimators=10, verbose=1)\n",
    "gbr_pca_opt.fit(x_train_pca, y_train)\n",
    "gbr_pca_opt.score(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': [0.1, 0.2, 0.4],\n",
    "          'min_samples_split': [2, 3, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 124488207080.0117            1.56s\n",
      "         2 111535586159.2763            1.63s\n",
      "         3 100968999671.4159            1.17s\n",
      "         4 92060216533.3797            0.89s\n",
      "         5 84670429061.9953            0.67s\n",
      "         6 78376623973.6192            0.50s\n",
      "         7 72994691687.8049            0.35s\n",
      "         8 68510889632.4280            0.23s\n",
      "         9 64765323713.8118            0.11s\n",
      "        10 61651309718.2557            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 123485207226.6198            0.78s\n",
      "         2 110721535798.5718            0.72s\n",
      "         3 100094881122.3578            0.66s\n",
      "         4 91464491532.6912            0.58s\n",
      "         5 84260967077.7431            0.48s\n",
      "         6 77979788259.0272            0.38s\n",
      "         7 72896990153.7111            0.29s\n",
      "         8 68689056404.8197            0.20s\n",
      "         9 64871092826.4461            0.11s\n",
      "        10 61507158621.1300            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 126676201413.9928            1.13s\n",
      "         2 113615111867.0228            0.86s\n",
      "         3 102927011174.1247            0.70s\n",
      "         4 93825137224.6373            0.59s\n",
      "         5 86279314191.2889            0.48s\n",
      "         6 79960524497.5012            0.37s\n",
      "         7 74593283426.5607            0.28s\n",
      "         8 70020384224.4095            0.18s\n",
      "         9 66198859336.8458            0.09s\n",
      "        10 62788365576.2421            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 122313526416.2199            0.73s\n",
      "         2 109576440075.2486            0.76s\n",
      "         3 99050971771.6372            0.68s\n",
      "         4 90606297285.5135            0.57s\n",
      "         5 83366036022.3642            0.47s\n",
      "         6 77092885483.5738            0.38s\n",
      "         7 71923754812.2309            0.28s\n",
      "         8 67618447239.7468            0.18s\n",
      "         9 63833406710.0061            0.09s\n",
      "        10 60594724625.4666            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 116086405780.2536            0.76s\n",
      "         2 104625437785.2832            0.67s\n",
      "         3 95007955457.1177            0.61s\n",
      "         4 86913641135.9776            0.52s\n",
      "         5 80175095357.9972            0.43s\n",
      "         6 74569235964.8933            0.36s\n",
      "         7 69744163708.3166            0.28s\n",
      "         8 65738526061.9274            0.19s\n",
      "         9 62489135788.2859            0.09s\n",
      "        10 59595870272.7089            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 124488207080.0117            0.74s\n",
      "         2 111535586159.2762            0.66s\n",
      "         3 100968999671.4159            0.63s\n",
      "         4 92060216533.3797            0.77s\n",
      "         5 84670429061.9953            0.62s\n",
      "         6 78376623973.6192            0.50s\n",
      "         7 72994691687.8049            0.37s\n",
      "         8 68510889632.4279            0.26s\n",
      "         9 64765323713.8118            0.13s\n",
      "        10 61651309718.2557            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 123485207226.6198            1.02s\n",
      "         2 110721535798.5718            1.01s\n",
      "         3 100094881122.3578            0.81s\n",
      "         4 91464491532.6912            0.65s\n",
      "         5 84260967077.7431            0.53s\n",
      "         6 77979788259.0272            0.41s\n",
      "         7 72896990153.7110            0.33s\n",
      "         8 68689056404.8197            0.25s\n",
      "         9 64871092826.4461            0.13s\n",
      "        10 61507158621.1300            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 126676201413.9928            1.08s\n",
      "         2 113615111867.0228            1.12s\n",
      "         3 102927011174.1248            0.92s\n",
      "         4 93825137224.6373            0.77s\n",
      "         5 86279314191.2890            0.65s\n",
      "         6 79960524497.5012            0.50s\n",
      "         7 74593283426.5608            0.38s\n",
      "         8 70020384224.4095            0.26s\n",
      "         9 66198859336.8458            0.13s\n",
      "        10 62788365576.2421            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 122313526416.2199            1.39s\n",
      "         2 109576440075.2487            1.10s\n",
      "         3 99050971771.6372            1.12s\n",
      "         4 90606297285.5136            0.95s\n",
      "         5 83366036022.3642            0.75s\n",
      "         6 77092885483.5738            0.57s\n",
      "         7 71923754812.2309            0.41s\n",
      "         8 67618447239.7468            0.27s\n",
      "         9 63833406710.0062            0.13s\n",
      "        10 60594724625.4666            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 116086405780.2536            0.98s\n",
      "         2 104625437785.2832            1.28s\n",
      "         3 95007955457.1177            1.12s\n",
      "         4 86913641135.9777            0.91s\n",
      "         5 80175095357.9972            0.82s\n",
      "         6 74569235964.8934            0.62s\n",
      "         7 69744163708.3166            0.52s\n",
      "         8 65738526061.9274            0.33s\n",
      "         9 62489135788.2859            0.16s\n",
      "        10 59595870272.7089            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 124488207080.0117            0.89s\n",
      "         2 111535586159.2762            0.82s\n",
      "         3 100968999671.4159            0.68s\n",
      "         4 92060216533.3797            0.72s\n",
      "         5 84670429061.9953            0.58s\n",
      "         6 78376623973.6192            0.45s\n",
      "         7 72994691687.8049            0.35s\n",
      "         8 68510889632.4279            0.22s\n",
      "         9 64765323713.8118            0.11s\n",
      "        10 61651309718.2557            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 123485207226.6199            0.83s\n",
      "         2 110721535798.5717            0.71s\n",
      "         3 100094881122.3577            0.64s\n",
      "         4 91464491532.6912            0.68s\n",
      "         5 84260967077.7430            0.61s\n",
      "         6 77979788259.0272            0.52s\n",
      "         7 72896990153.7110            0.39s\n",
      "         8 68689056404.8196            0.25s\n",
      "         9 64871092826.4461            0.13s\n",
      "        10 61507158621.1299            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 126676201413.9928            0.95s\n",
      "         2 113615111867.0228            0.81s\n",
      "         3 102927011174.1247            0.67s\n",
      "         4 93825137224.6372            0.56s\n",
      "         5 86279314191.2889            0.47s\n",
      "         6 79960524497.5012            0.37s\n",
      "         7 74593283426.5608            0.28s\n",
      "         8 70020384224.4095            0.18s\n",
      "         9 66198859336.8458            0.09s\n",
      "        10 62788365576.2421            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 122313526416.2198            0.79s\n",
      "         2 109576440075.2487            0.86s\n",
      "         3 99050971771.6372            0.84s\n",
      "         4 90606297285.5136            0.79s\n",
      "         5 83366036022.3642            0.67s\n",
      "         6 77092885483.5738            0.53s\n",
      "         7 71923754812.2309            0.41s\n",
      "         8 67618447239.7468            0.28s\n",
      "         9 63833406710.0061            0.14s\n",
      "        10 60594724625.4665            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 116086405780.2536            1.14s\n",
      "         2 104625437785.2832            1.04s\n",
      "         3 95007955457.1177            0.88s\n",
      "         4 86913641135.9776            0.70s\n",
      "         5 80175095357.9972            0.57s\n",
      "         6 74569235964.8933            0.45s\n",
      "         7 69744163708.3166            0.36s\n",
      "         8 65738526061.9274            0.25s\n",
      "         9 62489135788.2859            0.12s\n",
      "        10 59595870272.7089            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 110634853401.5807            0.93s\n",
      "         2 90726801084.7470            0.90s\n",
      "         3 77049834890.3586            0.76s\n",
      "         4 67228715134.1947            0.65s\n",
      "         5 60277855125.0765            0.56s\n",
      "         6 55160430845.0822            0.45s\n",
      "         7 51554749400.7676            0.33s\n",
      "         8 48610066146.4059            0.22s\n",
      "         9 46472673252.6216            0.11s\n",
      "        10 44604850631.6791            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 110110421836.1575            0.78s\n",
      "         2 89821885244.5390            0.74s\n",
      "         3 76584498150.9151            0.66s\n",
      "         4 67237605136.5816            0.57s\n",
      "         5 60379418287.0308            0.56s\n",
      "         6 55373358325.2755            0.46s\n",
      "         7 51538284809.3154            0.35s\n",
      "         8 48887846618.3566            0.24s\n",
      "         9 46847577491.7299            0.12s\n",
      "        10 45181556878.5300            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 112741113787.9318            1.00s\n",
      "         2 92189247706.0556            0.97s\n",
      "         3 78216649486.7685            0.81s\n",
      "         4 68453715293.8317            0.67s\n",
      "         5 61427533916.7547            0.55s\n",
      "         6 56398803227.3889            0.43s\n",
      "         7 52447603419.5892            0.32s\n",
      "         8 49688797512.5906            0.21s\n",
      "         9 47349253354.3981            0.11s\n",
      "        10 45645692746.9375            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 108923639942.6846            1.63s\n",
      "         2 88842929700.3405            1.36s\n",
      "         3 75666637666.1290            1.12s\n",
      "         4 66159359842.8396            0.90s\n",
      "         5 59404321606.1984            0.73s\n",
      "         6 54648762349.4092            0.59s\n",
      "         7 50720119561.1647            0.43s\n",
      "         8 48034788398.0326            0.28s\n",
      "         9 45819737362.0555            0.14s\n",
      "        10 44228859640.6950            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 103616979114.9926            0.89s\n",
      "         2 86114363720.1821            0.79s\n",
      "         3 73368629681.7119            0.67s\n",
      "         4 64705343338.6175            0.57s\n",
      "         5 58571319715.1503            0.47s\n",
      "         6 54109506490.5283            0.38s\n",
      "         7 50721194287.2498            0.28s\n",
      "         8 47981038406.6954            0.18s\n",
      "         9 45768305992.9115            0.10s\n",
      "        10 44109057629.3986            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 110634853401.5807            0.88s\n",
      "         2 90726801084.7470            0.76s\n",
      "         3 77049834890.3586            0.67s\n",
      "         4 67228715134.1947            0.56s\n",
      "         5 60277855125.0765            0.46s\n",
      "         6 55160430845.0822            0.37s\n",
      "         7 51554749400.7676            0.27s\n",
      "         8 48610066146.4059            0.18s\n",
      "         9 46472673252.6215            0.10s\n",
      "        10 44604850631.6791            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 110110421836.1575            1.02s\n",
      "         2 89821885244.5390            1.08s\n",
      "         3 76584498150.9151            0.92s\n",
      "         4 67237605136.5816            0.74s\n",
      "         5 60379418287.0308            0.62s\n",
      "         6 55373358325.2755            0.48s\n",
      "         7 51538284809.3154            0.37s\n",
      "         8 48887846618.3566            0.24s\n",
      "         9 46847577491.7299            0.12s\n",
      "        10 45181556878.5300            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 112741113787.9318            0.83s\n",
      "         2 92189247706.0555            0.77s\n",
      "         3 78216649486.7684            0.68s\n",
      "         4 68453715293.8316            0.58s\n",
      "         5 61427533916.7546            0.48s\n",
      "         6 56398803227.3889            0.39s\n",
      "         7 52447603419.5892            0.32s\n",
      "         8 49688797512.5906            0.23s\n",
      "         9 47349253354.3981            0.11s\n",
      "        10 45645692746.9374            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 108923639942.6846            0.94s\n",
      "         2 88842929700.3405            0.75s\n",
      "         3 75666637666.1290            0.67s\n",
      "         4 66159359842.8397            0.58s\n",
      "         5 59404321606.1984            0.57s\n",
      "         6 54648762349.4092            0.45s\n",
      "         7 50720119561.1647            0.33s\n",
      "         8 48034788398.0326            0.22s\n",
      "         9 45819737362.0555            0.11s\n",
      "        10 44228859640.6950            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 103616979114.9926            0.84s\n",
      "         2 86114363720.1821            0.74s\n",
      "         3 73368629681.7119            0.63s\n",
      "         4 64705343338.6175            0.53s\n",
      "         5 58571319715.1503            0.45s\n",
      "         6 54109506490.5283            0.36s\n",
      "         7 50721194287.2498            0.27s\n",
      "         8 47981038406.6954            0.18s\n",
      "         9 45768305992.9115            0.09s\n",
      "        10 44109057629.3986            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 110634853401.5807            0.83s\n",
      "         2 90726801084.7470            0.70s\n",
      "         3 77049834890.3586            0.62s\n",
      "         4 67228715134.1947            0.55s\n",
      "         5 60277855125.0765            0.46s\n",
      "         6 55160430845.0822            0.44s\n",
      "         7 51554749400.7676            0.33s\n",
      "         8 48610066146.4059            0.22s\n",
      "         9 46472673252.6216            0.11s\n",
      "        10 44604850631.6791            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 110110421836.1575            0.97s\n",
      "         2 89821885244.5390            0.83s\n",
      "         3 76584498150.9151            0.72s\n",
      "         4 67237605136.5816            0.63s\n",
      "         5 60379418287.0308            0.53s\n",
      "         6 55373358325.2755            0.43s\n",
      "         7 51538284809.3154            0.32s\n",
      "         8 48887846618.3566            0.22s\n",
      "         9 46847577491.7299            0.11s\n",
      "        10 45181556878.5300            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 112741113787.9318            0.88s\n",
      "         2 92189247706.0555            0.76s\n",
      "         3 78216649486.7685            0.74s\n",
      "         4 68453715293.8317            0.67s\n",
      "         5 61427533916.7547            0.56s\n",
      "         6 56398803227.3889            0.44s\n",
      "         7 52447603419.5892            0.33s\n",
      "         8 49688797512.5906            0.22s\n",
      "         9 47349253354.3981            0.11s\n",
      "        10 45645692746.9374            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 108923639942.6846            0.78s\n",
      "         2 88842929700.3405            0.71s\n",
      "         3 75666637666.1289            0.64s\n",
      "         4 66159359842.8396            0.54s\n",
      "         5 59404321606.1984            0.45s\n",
      "         6 54648762349.4092            0.36s\n",
      "         7 50720119561.1647            0.27s\n",
      "         8 48034788398.0326            0.18s\n",
      "         9 45819737362.0555            0.09s\n",
      "        10 44228859640.6950            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 103616979114.9926            0.71s\n",
      "         2 86114363720.1821            0.65s\n",
      "         3 73368629681.7119            0.58s\n",
      "         4 64705343338.6175            0.64s\n",
      "         5 58571319715.1503            0.52s\n",
      "         6 54109506490.5283            0.42s\n",
      "         7 50721194287.2498            0.32s\n",
      "         8 48002508781.7366            0.22s\n",
      "         9 45789776367.9528            0.11s\n",
      "        10 44129668560.1754            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 87817564990.0472            0.96s\n",
      "         2 65058102653.8049            0.77s\n",
      "         3 53618933352.7120            0.78s\n",
      "         4 48581286005.4258            0.65s\n",
      "         5 45427103962.7582            0.52s\n",
      "         6 43118847988.2225            0.43s\n",
      "         7 41711629534.5741            0.32s\n",
      "         8 40530411949.1172            0.20s\n",
      "         9 39833776529.3797            0.10s\n",
      "        10 39314293763.7881            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 88081363545.9843            0.80s\n",
      "         2 64672462064.2693            0.73s\n",
      "         3 54531744609.1350            0.68s\n",
      "         4 48314582318.6843            0.59s\n",
      "         5 45377827640.8703            0.48s\n",
      "         6 43393484573.5604            0.38s\n",
      "         7 41810515189.3935            0.28s\n",
      "         8 40558397542.9726            0.19s\n",
      "         9 39672350523.2455            0.10s\n",
      "        10 39194496402.8790            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 89789204756.7724            0.69s\n",
      "         2 65888410576.4777            0.67s\n",
      "         3 55110126207.0779            0.58s\n",
      "         4 49321841613.5107            0.54s\n",
      "         5 45339100479.1525            0.47s\n",
      "         6 43365016055.9403            0.40s\n",
      "         7 41748794411.9891            0.29s\n",
      "         8 40876100927.4964            0.20s\n",
      "         9 39899263599.4460            0.10s\n",
      "        10 39356416080.5100            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 86869709280.3911            0.95s\n",
      "         2 63703517461.0633            0.76s\n",
      "         3 53080452461.8123            0.65s\n",
      "         4 47236664719.6546            0.55s\n",
      "         5 44071742187.8263            0.45s\n",
      "         6 41894620995.9061            0.35s\n",
      "         7 40323483871.8747            0.26s\n",
      "         8 39254894586.0069            0.17s\n",
      "         9 38626788197.7548            0.09s\n",
      "        10 37938679572.9719            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 83079099901.6215            0.74s\n",
      "         2 63078490187.4215            0.65s\n",
      "         3 52705821366.6994            0.56s\n",
      "         4 47185414888.9943            0.49s\n",
      "         5 43810045081.5437            0.47s\n",
      "         6 41935789713.8217            0.39s\n",
      "         7 40698268171.1658            0.30s\n",
      "         8 40196786559.6585            0.20s\n",
      "         9 39460924191.5640            0.10s\n",
      "        10 38604131019.4969            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 87817564990.0472            0.74s\n",
      "         2 65058102653.8049            0.66s\n",
      "         3 53618933352.7120            0.57s\n",
      "         4 48581286005.4258            0.50s\n",
      "         5 45427103962.7582            0.46s\n",
      "         6 43118847988.2225            0.39s\n",
      "         7 41711629534.5741            0.30s\n",
      "         8 40530411949.1172            0.20s\n",
      "         9 39833776529.3797            0.10s\n",
      "        10 39314293763.7881            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 88081363545.9843            0.74s\n",
      "         2 64672462064.2692            0.65s\n",
      "         3 54531744609.1350            0.56s\n",
      "         4 48314582318.6843            0.49s\n",
      "         5 45377827640.8703            0.41s\n",
      "         6 43393484573.5604            0.36s\n",
      "         7 41810515189.3935            0.27s\n",
      "         8 40558397542.9726            0.19s\n",
      "         9 39672350523.2455            0.09s\n",
      "        10 39194496402.8790            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 89789204756.7725            0.80s\n",
      "         2 65888410576.4777            0.72s\n",
      "         3 55110126207.0779            0.65s\n",
      "         4 49321841613.5107            0.58s\n",
      "         5 45339100479.1525            0.57s\n",
      "         6 43365016055.9403            0.48s\n",
      "         7 41748794411.9891            0.35s\n",
      "         8 40876100927.4964            0.23s\n",
      "         9 39899263599.4460            0.11s\n",
      "        10 39356477426.0792            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 86869709280.3911            0.92s\n",
      "         2 63703517461.0635            0.74s\n",
      "         3 53080452461.8123            0.62s\n",
      "         4 47236664719.6546            0.51s\n",
      "         5 44071742187.8263            0.45s\n",
      "         6 41894620995.9061            0.36s\n",
      "         7 40323483871.8747            0.27s\n",
      "         8 39254894586.0069            0.18s\n",
      "         9 38626788197.7548            0.09s\n",
      "        10 37938679572.9718            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 83079099901.6216            0.81s\n",
      "         2 63078490187.4215            0.73s\n",
      "         3 52705821366.6994            0.68s\n",
      "         4 47185414888.9943            0.57s\n",
      "         5 43810045081.5437            0.46s\n",
      "         6 41935789713.8217            0.42s\n",
      "         7 40698268171.1658            0.31s\n",
      "         8 40198135248.6117            0.21s\n",
      "         9 39462272880.5171            0.10s\n",
      "        10 38601470967.8720            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 87817564990.0472            0.86s\n",
      "         2 65058102653.8049            0.77s\n",
      "         3 53618933352.7120            0.64s\n",
      "         4 48581286005.4258            0.53s\n",
      "         5 45427103962.7582            0.44s\n",
      "         6 43118847988.2225            0.37s\n",
      "         7 41711629534.5741            0.27s\n",
      "         8 40530411949.1172            0.18s\n",
      "         9 39833776529.3797            0.09s\n",
      "        10 39314293763.7881            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 88081363545.9843            0.72s\n",
      "         2 64672462064.2693            0.67s\n",
      "         3 54531744609.1350            0.60s\n",
      "         4 48314582318.6843            0.53s\n",
      "         5 45440205738.8847            0.44s\n",
      "         6 43477969774.9085            0.35s\n",
      "         7 41871782731.6444            0.30s\n",
      "         8 40615854255.1148            0.20s\n",
      "         9 39724508913.8594            0.10s\n",
      "        10 39378460352.9824            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 89789204756.7724            1.03s\n",
      "         2 65888410576.4777            0.88s\n",
      "         3 55110126207.0780            0.70s\n",
      "         4 49321841613.5108            0.58s\n",
      "         5 45339100479.1525            0.46s\n",
      "         6 43365016055.9403            0.37s\n",
      "         7 41748794411.9891            0.28s\n",
      "         8 40905261827.3715            0.19s\n",
      "         9 39927417865.9820            0.10s\n",
      "        10 39354798732.3994            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 86869709280.3911            0.76s\n",
      "         2 63703517461.0633            0.70s\n",
      "         3 53080452461.8124            0.60s\n",
      "         4 47236664719.6546            0.53s\n",
      "         5 44071742187.8263            0.43s\n",
      "         6 41894620995.9061            0.34s\n",
      "         7 40323483871.8747            0.27s\n",
      "         8 39254894586.0069            0.19s\n",
      "         9 38628509041.5052            0.09s\n",
      "        10 37940400416.7222            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 83079099901.6216            0.90s\n",
      "         2 63078490187.4215            0.79s\n",
      "         3 52705821366.6994            0.71s\n",
      "         4 47230812842.1502            0.60s\n",
      "         5 43862358257.7016            0.47s\n",
      "         6 41919871042.2267            0.37s\n",
      "         7 40602649869.6263            0.29s\n",
      "         8 39903595640.8537            0.19s\n",
      "         9 39257763995.4375            0.09s\n",
      "        10 38585499017.2961            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 88014797103.5459            1.05s\n",
      "         2 65045881213.4452            0.93s\n",
      "         3 54962235784.4456            0.82s\n",
      "         4 49394434894.2775            0.70s\n",
      "         5 46293655219.9269            0.58s\n",
      "         6 44004804080.0326            0.53s\n",
      "         7 42457292733.8240            0.40s\n",
      "         8 41393133719.6774            0.26s\n",
      "         9 40733855115.1441            0.13s\n",
      "        10 39789812457.5718            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                                 criterion='mse', init=None,\n",
       "                                                 learning_rate=0.1, loss='ls',\n",
       "                                                 max_depth=3, max_features=None,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_split=2,\n",
       "                                                 min_weight_fraction_leaf=0.0,\n",
       "                                                 n_estimators=10,\n",
       "                                                 n_iter_no_change=None,\n",
       "                                                 presort='deprecated',\n",
       "                                                 random_state=None,\n",
       "                                                 subsample=1.0, tol=0.0001,\n",
       "                                                 validation_fraction=0.1,\n",
       "                                                 verbose=1, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'learning_rate': [0.1, 0.2, 0.4],\n",
       "                         'min_samples_split': [2, 3, 5]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(gbr_pca_opt, params)\n",
    "gs.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='mse', init=None,\n",
       "                          learning_rate=0.4, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=3,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search best_estimator_ output model (VERY OVERFIT)\n",
    "\n",
    "# GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='mse', init=None,\n",
    "#                           learning_rate=0.4, loss='ls', max_depth=3,\n",
    "#                           max_features=None, max_leaf_nodes=None,\n",
    "#                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                           min_samples_leaf=1, min_samples_split=3,\n",
    "#                           min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "#                           n_iter_no_change=None, presort='deprecated',\n",
    "#                           random_state=None, subsample=1.0, tol=0.0001,\n",
    "#                           validation_fraction=0.1, verbose=1, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 125493800661.5281           22.97s\n",
      "         2 113543622057.6961           26.29s\n",
      "         3 103594050779.2841           26.14s\n",
      "         4 95304720494.4557           24.34s\n",
      "         5 88389149079.0906           23.05s\n",
      "         6 82441451858.9953           22.41s\n",
      "         7 76963870538.3639           21.85s\n",
      "         8 72262634708.7123           21.24s\n",
      "         9 68267160870.0110           20.68s\n",
      "        10 64803015487.1105           20.23s\n",
      "        20 46201309021.4866           17.95s\n",
      "        30 38629264324.6173           17.56s\n",
      "        40 34755949376.7570           17.16s\n",
      "        50 32430723631.0676           16.57s\n",
      "        60 30998711559.9802           16.35s\n",
      "        70 29812066330.3973           15.83s\n",
      "        80 28849694439.4422           15.37s\n",
      "        90 28022442732.7863           15.17s\n",
      "       100 27325185565.0321           14.79s\n",
      "       200 23285518375.4471           11.21s\n",
      "       300 21003408104.2001            7.51s\n",
      "       400 19417847791.9554            3.76s\n",
      "       500 18165068887.2306            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7635080861129032"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_pca = GradientBoostingRegressor(n_estimators=500,  min_samples_split=3, verbose=1)\n",
    "gbr_pca.fit(x_train2, y_train2)\n",
    "gbr_pca.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 88708785196.1104            1.79m\n",
      "         2 65295883566.9057            1.50m\n",
      "         3 54949160123.2202            1.32m\n",
      "         4 48851259209.9144            1.21m\n",
      "         5 46060424240.8074            1.14m\n",
      "         6 43982121904.4224            1.09m\n",
      "         7 42408502389.2186            1.04m\n",
      "         8 41226848788.1739            1.02m\n",
      "         9 40306846834.7694            1.01m\n",
      "        10 39823513586.2918           59.48s\n",
      "        20 35822877815.1472           56.19s\n",
      "        30 33176704060.1389           54.99s\n",
      "        40 31023832239.6091           52.61s\n",
      "        50 29863353124.1103           50.79s\n",
      "        60 28235389785.2225           50.04s\n",
      "        70 27269054186.7039           48.80s\n",
      "        80 26394036596.4370           49.15s\n",
      "        90 25542363316.2314           50.01s\n",
      "       100 24731362543.0808           49.45s\n",
      "       200 18804800794.5928           43.94s\n",
      "       300 15602345655.7715           38.31s\n",
      "       400 13250772174.1184           32.49s\n",
      "       500 11534610012.4018           26.96s\n",
      "       600 10115997541.0677           21.47s\n",
      "       700  8935408570.5742           16.06s\n",
      "       800  7897264860.3296           10.67s\n",
      "       900  7024454214.3319            5.33s\n",
      "      1000  6190267706.3087            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9557792502732567"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_pca.fit(x_train_pca2, y_train2)\n",
    "gbr_pca.score(x_train_pca2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78678.25434202689"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms2_train = sqrt(mean_squared_error(y_train2, gbr_pca.predict(x_train_pca2)))\n",
    "rms2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 88708785196.1104            1.39m\n",
      "         2 65295883566.9057            1.32m\n",
      "         3 54949160123.2202            1.15m\n",
      "         4 48851259209.9143            1.08m\n",
      "         5 46060424240.8074            1.03m\n",
      "         6 43982121904.4224            1.08m\n",
      "         7 42408502389.2186            1.05m\n",
      "         8 41226848788.1739            1.03m\n",
      "         9 40306846834.7694            1.01m\n",
      "        10 39823513586.2918           59.21s\n",
      "        20 35822877815.1472           50.99s\n",
      "        30 33176704060.1389           49.71s\n",
      "        40 31023832239.6091           47.74s\n",
      "        50 29863353124.1103           47.09s\n",
      "        60 28235389785.2225           46.55s\n",
      "        70 27269054186.7039           46.18s\n",
      "        80 26394036596.4370           45.38s\n",
      "        90 25542363316.2314           44.93s\n",
      "       100 24731362543.0808           44.57s\n",
      "       200 18804800794.5928           41.79s\n",
      "       300 15602345655.7715           38.04s\n",
      "       400 13250772174.1184           33.33s\n",
      "       500 11534610012.4018           28.06s\n",
      "       600 10115997541.0677           22.55s\n",
      "       700  8935408570.5742           16.88s\n",
      "       800  7897264860.3296           11.24s\n",
      "       900  7024454214.3319            5.62s\n",
      "      1000  6190267706.3087            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.7331904052099714"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_pca.fit(x_train_pca2, y_train2)\n",
    "gbr_pca.score(x_test_pca2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms2_test = sqrt(mean_squared_error(y_test2, gbr_pca.predict(x_test_pca2)))\n",
    "rms2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_train, gbr_pca.predict(x_train_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75402.01472962025"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 87817564990.0472            1.34m\n",
      "         2 65058102653.8049            1.51m\n",
      "         3 53618933352.7120            1.47m\n",
      "         4 48581286005.4258            1.31m\n",
      "         5 45427103962.7582            1.19m\n",
      "         6 43118847988.2225            1.10m\n",
      "         7 41711629534.5741            1.05m\n",
      "         8 40530411949.1172            1.01m\n",
      "         9 39833776529.3797           58.87s\n",
      "        10 39314293763.7881           57.33s\n",
      "        20 35350118220.5889           50.69s\n",
      "        30 32609109381.3944           48.91s\n",
      "        40 29814877791.7152           49.62s\n",
      "        50 27980268238.0792           50.51s\n",
      "        60 26332951025.2575           48.83s\n",
      "        70 25118813322.7857           47.35s\n",
      "        80 24052066848.0023           46.39s\n",
      "        90 22927199930.8862           45.68s\n",
      "       100 22211426986.8578           44.86s\n",
      "       200 16274323677.5728           38.65s\n",
      "       300 13040128615.4458           34.46s\n",
      "       400 10757069971.9321           30.22s\n",
      "       500  9201336086.1031           25.82s\n",
      "       600  7788147119.0343           20.72s\n",
      "       700  6693133074.3448           15.38s\n",
      "       800  5731376944.8020           10.20s\n",
      "       900  4945214610.0230            5.08s\n",
      "      1000  4284499546.4469            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 88081363545.9843           49.03s\n",
      "         2 64672462064.2693           47.89s\n",
      "         3 54531744609.1351           47.89s\n",
      "         4 48314582318.6843           48.83s\n",
      "         5 45377827640.8703           48.83s\n",
      "         6 43393484573.5604           48.64s\n",
      "         7 41810515189.3935           48.80s\n",
      "         8 40558397542.9726           50.81s\n",
      "         9 39672350523.2455           52.54s\n",
      "        10 39194496402.8790           52.46s\n",
      "        20 34439393092.1150           49.28s\n",
      "        30 31407855041.5972           48.72s\n",
      "        40 29247374308.5855           48.15s\n",
      "        50 27623989156.6175           48.24s\n",
      "        60 26351218676.1805           47.60s\n",
      "        70 24958883735.4381           47.12s\n",
      "        80 23962139897.7621           46.94s\n",
      "        90 23012708852.7082           46.58s\n",
      "       100 22166908378.7763           45.61s\n",
      "       200 16377434030.2478           40.17s\n",
      "       300 13212017374.5509           35.38s\n",
      "       400 10842068159.8843           30.32s\n",
      "       500  9160227684.1514           25.32s\n",
      "       600  7739432647.9793           20.29s\n",
      "       700  6621364772.2687           15.30s\n",
      "       800  5699213730.0975           10.26s\n",
      "       900  4953906849.6378            5.16s\n",
      "      1000  4361263233.7193            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 89789204756.7725           53.28s\n",
      "         2 65888410576.4778           53.42s\n",
      "         3 55110126207.0780           53.44s\n",
      "         4 49321841613.5108           53.50s\n",
      "         5 45339100479.1525           52.64s\n",
      "         6 43365016055.9403           51.97s\n",
      "         7 41748794411.9891           51.54s\n",
      "         8 40876100927.4964           51.25s\n",
      "         9 39899263599.4460           51.09s\n",
      "        10 39356477426.0792           50.89s\n",
      "        20 34821480937.3584           52.80s\n",
      "        30 31976642106.3682           51.30s\n",
      "        40 29478876843.2787           50.91s\n",
      "        50 27503346359.7173           49.89s\n",
      "        60 26308094906.7309           49.83s\n",
      "        70 24998681155.9699           49.05s\n",
      "        80 23913703001.0932           48.82s\n",
      "        90 22947285213.0861           48.57s\n",
      "       100 21892576214.5461           48.18s\n",
      "       200 16270217048.2214           43.15s\n",
      "       300 13144362745.7824           37.92s\n",
      "       400 10996255415.0438           32.13s\n",
      "       500  9331674486.0596           26.57s\n",
      "       600  7940286747.6950           21.13s\n",
      "       700  6892323369.3534           15.78s\n",
      "       800  5972936014.5098           10.48s\n",
      "       900  5189150579.7916            5.23s\n",
      "      1000  4522305847.8825            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 86869709280.3911            1.01m\n",
      "         2 63703517461.0634           59.32s\n",
      "         3 53080452461.8123            1.02m\n",
      "         4 47236664719.6546            1.00m\n",
      "         5 44071742187.8263            1.00m\n",
      "         6 41894620995.9061           58.88s\n",
      "         7 40323483871.8747           58.42s\n",
      "         8 39254894586.0069           58.63s\n",
      "         9 38626788197.7548           58.35s\n",
      "        10 37938679572.9718           57.70s\n",
      "        20 34201866635.4656            1.02m\n",
      "        30 31456146769.7369            1.03m\n",
      "        40 28663280380.7818            1.02m\n",
      "        50 27003674673.9731            1.01m\n",
      "        60 25359514585.9215            1.00m\n",
      "        70 24319196154.6743           59.23s\n",
      "        80 23189397738.2547           58.88s\n",
      "        90 22299315934.6969           57.89s\n",
      "       100 21464590643.9930           57.12s\n",
      "       200 15881300223.1190           48.98s\n",
      "       300 12801780152.5893           41.42s\n",
      "       400 10627880206.8922           34.41s\n",
      "       500  8863787027.4760           28.04s\n",
      "       600  7607665364.8839           22.16s\n",
      "       700  6448364260.5332           16.42s\n",
      "       800  5536985685.5402           10.79s\n",
      "       900  4835528859.3927            5.33s\n",
      "      1000  4179953967.8185            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1 83079099901.6215           48.06s\n",
      "         2 63078490187.4215           48.55s\n",
      "         3 52705821366.6994           48.35s\n",
      "         4 47185414888.9943           49.27s\n",
      "         5 43810045081.5437           49.93s\n",
      "         6 41935789713.8217           49.74s\n",
      "         7 40698268171.1658           49.53s\n",
      "         8 40198135248.6117           49.30s\n",
      "         9 39462272880.5171           49.38s\n",
      "        10 38601470967.8720           49.34s\n",
      "        20 34764547304.4905           50.81s\n",
      "        30 31634232106.8411           49.35s\n",
      "        40 29340365301.6008           50.47s\n",
      "        50 27469356221.0131           50.31s\n",
      "        60 25769315759.2672           49.55s\n",
      "        70 24305677962.6762           49.45s\n",
      "        80 23156757054.2450           48.93s\n",
      "        90 22414865643.1316           48.62s\n",
      "       100 21560261004.2679           47.80s\n",
      "       200 16012062903.5139           41.63s\n",
      "       300 12667882933.9692           36.27s\n",
      "       400 10288355609.5381           30.88s\n",
      "       500  8660561558.6551           25.63s\n",
      "       600  7351631567.3573           20.51s\n",
      "       700  6295045057.2738           15.53s\n",
      "       800  5411893485.5006           10.47s\n",
      "       900  4587061952.9841            5.31s\n",
      "      1000  3978313410.2058            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-214381.03554666, -217225.42452155, -208960.95322306,\n",
       "       -218479.51829197, -242828.33110526])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gbr_pca, x_train_pca, y_train, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sklearn.metrics.SCORERS.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
